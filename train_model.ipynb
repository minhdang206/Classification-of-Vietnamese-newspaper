{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import regex as re\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_csv('data_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news[df_news['label_id'] != 'label_id'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame([\n",
    "    {'label_id':'01', 'label': 'Đời Sống'},\n",
    "    {'label_id':'02', 'label': 'Du Lịch'},\n",
    "    {'label_id':'03', 'label': 'Giải Trí'},\n",
    "    {'label_id':'04', 'label': 'Giáo Dục'},\n",
    "    {'label_id':'05', 'label': 'Khoa Học'},\n",
    "    {'label_id':'06', 'label': 'Kinh Doanh'},\n",
    "    {'label_id':'07', 'label': 'Pháp Luật'},\n",
    "    {'label_id':'08', 'label': 'Sức Khỏe'},\n",
    "    {'label_id':'09', 'label': 'Thể Thao'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news.merge(df_labels,on=['label_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news[['title', 'content','label']].to_csv('data_news.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Một số hàm tiền xử lý văn bản cần thiết\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "stopword = set()\n",
    "file = r'vietnamese-stopwords.txt'\n",
    "for line in open(file, \"r\",encoding=\"utf8\"):\n",
    "  line = line.strip('\\n')\n",
    "  stopword.add(line)\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def text_preprocess(document):\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # xóa stopwords\n",
    "    document = remove_stopwords(document)\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['title'] = df_news['title'].apply(lambda x : text_preprocess(x))\n",
    "df_news['content'] = df_news['content'].apply(lambda x : text_preprocess(x))\n",
    "df_news['text'] = df_news['title'] + ' ' + df_news['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Du Lịch       630\n",
       "Giáo Dục      623\n",
       "Đời Sống      621\n",
       "Kinh Doanh    620\n",
       "Sức Khỏe      619\n",
       "Pháp Luật     617\n",
       "Thể Thao      616\n",
       "Giải Trí      616\n",
       "Khoa Học      608\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Thống kê các word xuất hiện ở tất cả các nhãn\n",
    "total_label = 9\n",
    "# Lấy túi từ vựng của dữ liệu đầu vào\n",
    "vocab = {}\n",
    "label_vocab = {}\n",
    "for idx, row in  df_news.iterrows():\n",
    "    words = row['text'].split()\n",
    "    label = row['label']\n",
    "    if label not in label_vocab:\n",
    "        label_vocab[label] = {}\n",
    "    for word in words:\n",
    "        label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
    "        if word not in vocab:\n",
    "            vocab[word] = set()\n",
    "        vocab[word].add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia tập train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "test_percent = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Du Lịch', 'Giáo Dục', 'Giải Trí', 'Khoa Học', 'Kinh Doanh', 'Pháp Luật', 'Sức Khỏe', 'Thể Thao', 'Đời Sống'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_news['text'], df_news['label'], test_size=test_percent, random_state=42)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "label_encoder.fit(y_test)\n",
    "print(list(label_encoder.classes_), '\\n')\n",
    "Y_train = label_encoder.transform(y_train)\n",
    "Y_test = label_encoder.transform(y_test)\n",
    "Y_train = np_utils.to_categorical(Y_train, 9)\n",
    "Y_test = np_utils.to_categorical(Y_test, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label = {}\n",
    "for idx, val in enumerate(list(label_encoder.classes_)):\n",
    "  dict_label[str(idx)] = val\n",
    "pickle.dump(dict_label,open('dict_label.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu thứ nhất: ngân_hàng quân_đội chủ_tịch quyết_định thay_đổi nhân_sự ngân_hàng quân_đội mb công_bố đồng_thời hội_đồng_quản_trị mb thông_qua thành_viên hội_đồng_quản_trị nhiệm_kỳ 2019 2024 11 10 thượng_tướng lê_hữu_đức chủ_tịch hội_đồng_quản_trị mb đánh_giá tổng công_trình_sư ngân_hàng dẫn_dắt mb 12 trải giai_đoạn chiến_lược dấu_ấn đậm_nét định_hướng chiến_lược xây_dựng nền_tảng quản_trị điều_hành lưu trung_thái sinh 1975 tốt_nghiệp thạc sỹ quản_trị kinh_doanh đại_học hawaii mỹ 26 gắn_bó mb mbs liền nắm vị_trí quan_trọng giám_đốc chi_nhánh giám_đốc nhân_sự phó tổng_giám_đốc thái bắt_đầu tham_gia hội_đồng_quản_trị mb 42013 trở_thành phó_chủ_tịch hội_đồng_quản_trị ngân_hàng 92013 giai_đoạn 2013 2017 tham_gia tái cấu_trúc công_ty thành_viên góp_phần xây_dựng hệ_sinh_thái mb thành tập_đoàn tài_chính đa_năng đầu 2017 bắt_đầu vị_trí ceo nhà_băng bổ_nhiệm vị_trí chủ_tịch hội_đồng_quản_trị mb thái đảm_nhận chức_danh ceo ngân_hàng thay phạm ánh phó tổng_giám_đốc giao phụ_trách ban điều_hành đảm_nhận quyền_hạn nhiệm_vụ tổng_giám_đốc phạm ánh sinh 1980 lớp lãnh_đạo trẻ mb 20 kinh_nghiệm ngành tài_chính ngân_hàng mb ánh đảm_nhiệm vị_trí giám_đốc chi_nhánh giám_đốc khối phụ_trách kinh_doanh khu_vực nam thành_viên ban điều_hành quyết_định thay_đổi nhân_sự hội_đồng_quản_trị mb trình cổ_đông thông_qua phiên họp thường_niên 2022 mb nhà_băng cổ_phần nằm dẫn_đầu lợi_nhuận lợi_nhuận thuế ngân_hàng 22 700 tỷ đồng 37 kỳ đứng vietcombank techcombank bidv nhà_băng ghi_nhận 20 triệu khách_hàng 2022 mb thu_hút 7 triệu khách_hàng số_lượng giao_dịch kênh vọt quy_mô 1 8_tỷ giao_dịch gấp 2 2021 _minh sơn\n",
      "Label câu thứ nhất: Kinh Doanh\n",
      "Độ dài:  60\n",
      "Tổng số câu 4456\n",
      "Sequences_matrix shape:  (4456, 1600)\n",
      "Sequences_matrix câu thứ nhất:  [  3 302 264 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "max_words = len(vocab.keys())\n",
    "\n",
    "tok = Tokenizer(num_words = max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "pickle.dump(tok,open('tokenize_text.pkl', 'wb'))\n",
    "\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "max_len = 1600\n",
    "\n",
    "sequences_matrix = pad_sequences(sequences,maxlen = max_len,padding='post')\n",
    "\n",
    "print('Câu thứ nhất:', X_train[1])\n",
    "print('Label câu thứ nhất:', y_train[1])\n",
    "print('Độ dài: ', len(sequences[1]))\n",
    "print('Tổng số câu', len(sequences))\n",
    "print('Sequences_matrix shape: ', sequences_matrix.shape)\n",
    "print('Sequences_matrix câu thứ nhất: ', sequences_matrix[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1600, 1000)        49000000  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1594, 128)         896128    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1588, 64)          57408     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1582, 28)          12572     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 28)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                522       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 171       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,966,801\n",
      "Trainable params: 49,966,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "126/126 [==============================] - 329s 3s/step - loss: 0.4271 - accuracy: 0.1581 - val_loss: 0.2837 - val_accuracy: 0.3206\n",
      "Epoch 2/10\n",
      "126/126 [==============================] - 280s 2s/step - loss: 0.2300 - accuracy: 0.5302 - val_loss: 0.2062 - val_accuracy: 0.6502\n",
      "Epoch 3/10\n",
      "126/126 [==============================] - 280s 2s/step - loss: 0.1432 - accuracy: 0.7870 - val_loss: 0.1660 - val_accuracy: 0.7399\n",
      "Epoch 4/10\n",
      "126/126 [==============================] - 280s 2s/step - loss: 0.0863 - accuracy: 0.8835 - val_loss: 0.1702 - val_accuracy: 0.7691\n",
      "Epoch 5/10\n",
      "126/126 [==============================] - 275s 2s/step - loss: 0.0592 - accuracy: 0.9294 - val_loss: 0.1891 - val_accuracy: 0.7691\n",
      "Epoch 6/10\n",
      "126/126 [==============================] - 276s 2s/step - loss: 0.0368 - accuracy: 0.9566 - val_loss: 0.2207 - val_accuracy: 0.7960\n",
      "Epoch 7/10\n",
      "126/126 [==============================] - 278s 2s/step - loss: 0.0227 - accuracy: 0.9773 - val_loss: 0.2254 - val_accuracy: 0.8117\n",
      "Epoch 8/10\n",
      "126/126 [==============================] - 277s 2s/step - loss: 0.0190 - accuracy: 0.9793 - val_loss: 0.2522 - val_accuracy: 0.8072\n",
      "Epoch 9/10\n",
      "126/126 [==============================] - 278s 2s/step - loss: 0.0230 - accuracy: 0.9781 - val_loss: 0.2372 - val_accuracy: 0.8027\n",
      "Epoch 10/10\n",
      "126/126 [==============================] - 280s 2s/step - loss: 0.0167 - accuracy: 0.9868 - val_loss: 0.2583 - val_accuracy: 0.7870\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_dim =1000\n",
    "num_filters = 128\n",
    "kernel_size = 7\n",
    "vocab_size = 49000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=1600))\n",
    "model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "model.add(layers.Conv1D(64, kernel_size, activation='relu'))\n",
    "model.add(layers.Conv1D(28, kernel_size, activation='relu'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(18, activation='relu'))\n",
    "model.add(layers.Dense(9, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(sequences_matrix, Y_train, validation_split = 0.1,\n",
    "batch_size=32, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19267897307872772, 0.8249551057815552]\n",
      "35/35 [==============================] - 15s 415ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "max_len = 1600\n",
    "\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen = max_len,padding='post')\n",
    "score = model.evaluate(test_sequences_matrix, Y_test, verbose=0)\n",
    "print(score)\n",
    "\n",
    "y_predict = model.predict(test_sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu test: tết đồ_thừa tết 20 mẹ chồng khánh vân bỉm_sơn thanh_hóa gọi điện_thông_báo đụng lợn 80 kg ngăn mẹ nàng dâu dặn tùy tình_hình cân_đối hai bận bốn người_lớn hai đứa tết mẹ chốt đụng lợn vợ_chồng gửi 5 triệu đồng sắm tết vân _bận kinh_doanh trưa 30 tết vợ_chồng bố_mẹ lo tươm_tất kết_hôn vân phát hoảng thực_phẩm đồ_ăn khắp nia bếp bày hai chục bánh_chưng tủ_lạnh chất 5 giò_lụa bếp nồi thịt đông riu_riu lửa nồi cá kho xong sân giếng treo giò_xào hai xâu nem_chua treo lủng_lẳng mẹ mua lá nem khéo lắm sợ rách bảo tủ_lạnh rán thả rán sơ mẹ vân 50 nem _lượng thực_phẩm đồ nấu xong thắp hương vân phụ mẹ chuẩn_bị cơm_cúng tàn_hương gia_đình hạ mâm đồ_ăn nguội_ngắt hai bữa đầu đồ ngon chán thành_thử món đạm bánh_chưng cúng ế bữa bữa mâm cơm_mới hộp túi nồi đựng đồ ăn_thừa ngày_một tiêu_thụ phần_nào thậm_chí đồ ăn_thừa vân đi tiếc bố_mẹ đi thậm_chí đổ_lộn giò thịt_luộc thịt đông nồi bữa đun mùng 3 tết bụng_dạ hầu_như đình_công bố_mẹ uống men tiêu_hóa mẹ đi ruộng bẻ ngô luộc nhẹ_bụng vợ_chồng mong vân tình_cảnh tích_trữ thực_phẩm tết đồ_thừa không_chỉ gia_đình vân khảo_sát 500 độc_giả vnexpress câu tình_trạng tích_trữ thực_phẩm tết 10 gia_đình tích_trữ thực_phẩm rằm giêng 41 tích_trữ kỳ nghỉ tết 49 gia_đình nguyễn tất_định 43 quê hải_dương đồ_thừa nổi câu_chuyện bao xung_đột tết con_trưởng quan_trọng mâm cơm_cúng 30 tết hóa_vàng hôm bữa cúng to bữa cúng đơn_giản cỗ cúng bữa ít_nhất món cơ_bản gà giò chả bát nấu đĩa xào bánh_chưng món rán bát đĩa kiểu bôi một_tí cúng cỗ mặn cúng cỗ chay cỗ cúng bữa phụ có_thể bớt đi giò chả bánh_chưng cỗ_bàn mẹ vất_vả 80 sức yếu chuẩn_bị mâm cỗ 12 15 món dậy 4 h30 cúng xong hạ mâm qua_loa dọn_dẹp nghỉ_ngơi chút tiếp_tục lao bếp chuẩn_bị mâm cỗ cúng bữa chiều vòng_quay bếp_núc lặp_lại kéo_dài tận mùng 4 tết mẹ bớt vất_vả định mua một_số đồ sẵn dặn mẹ bữa nóng cúng bếp đỡ_đần mẹ đi ngoại mẹ bố gia_trưởng bao_giờ thò tí mấy tết mẹ vất_vả chục định ý_kiến bố_mẹ cúng_lễ đơn_giản để_dành thời_gian nghỉ_ngơi ví_dụ bố_vợ trưởng cơm_cúng tất_niên hóa_vàng bố toàn mắng dằn_vặt mẹ hai ông_bà mẹ ức_chế mệt_mỏi _mẹ bảo bao_nhiêu quen cố đời yên cửa yên định mạng xã_hội 2 triệu thành_viên dịp tết xuất_hiện viết xử_lý đồ ăn_thừa hạnh quê phú thọ chia_sẻ xử_lý thịt_luộc tết thu_hút 3 000 lượt chia_sẻ tư_tưởng truyền_thống cúng hai mâm sân tết tủ_lạnh chật_cứng đồ thịt_luộc chục miếng dồn cúng suy_nghĩ xử_lý_số thịt cuối_cùng nem món thịt chua phú thọ gọi nem giải_quyết chỗ thịt_luộc dư_thừa trẻ_con_người_lớn nhậu ổn lắm ngon luộc thịt chín_tới thay_vì thịt_luộc sẵn tủ_lạnh chị_em chia_sẻ xử_lý thịt gà có_thể gỏi gà phở gà bún_thang cơm gà hội an _giò chả thừa chế_biến kho xôi bún_thang miến trộn thái sợi phở phó_giáo_sư bùi hoài_sơn ủy_ban văn_hóa giáo_dục quốc_hội tích_trữ tết phong_tục tập_quán việt đời thực_trạng đầu_tiên xuất_phát truyền_thống kiêng đồ tết kể_cả bối_cảnh xã_hội hiện_nay chúng_ta thuận_tiện mua_bán thói_quen truyền_thống no tết ấm hè việt mua đồ cảm_giác no_đủ gia_đình suy_nghĩ tết dông may_mắn dịp đầu xuân chúng_ta lễ cúng tất_niên giao_thừa cúng tết _lễ đòi_hỏi mâm cỗ thịnh_soạn mong_muốn thể_hiện gia_đình mâm cỗ linh_đình thịnh_soạn dịp tết mâm cỗ đầy_đủ thể_hiện chuẩn_bị dư_thừa khả_năng tiêu_thụ hầu_hết gia_đình đồ hai đầu toàn đồ_thừa cảm_giác ngon_miệng phó_giáo_sư thực_trạng có_thể thay_đổi ảnh_hưởng giá_trị tết phong_tục tập_quán chứa_đựng giá_trị quan_trọng chúng_ta giữ_gìn phong_tục tập_quán hình_thức tinh_thần truyền_thống hiếu lễ ý_nghĩa đoàn_viên giá_trị sum_họp gia_đình giá_trị quan_trọng thể_hiện_hình có_thể thay_đổi đặc_biệt bối_cảnh xã_hội điều_kiện chúng_ta lo_lắng giá_cả tất_định cỗ_bàn thừa_mứa thức_ăn tết thực_sự ám_ảnh đấu_tranh tuyên_bố bố_mẹ một_cách sống mẹ bố cúng_lễ thế_nào đơn_giản miễn_sao trọn_vẹn thay_đổi sống đời khổ con_cháu khổ _phan dương\n",
      "Giá trị dự đoán: Đời Sống\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f\"Câu test: {list(X_test)[i]}\\nGiá trị dự đoán: {dict_label[str(np.argmax(y_predict[0]))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_cnn_news.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
